{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle\n",
    "\n",
    "class WikiartDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, preprocess):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.preprocess = preprocess\n",
    "        self.context_length = 77\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = self.image_dir + row['relative_path']\n",
    "        image = self.preprocess(Image.open(image_path))\n",
    "\n",
    "        title = row['relative_path'].split('/')[-1].split('.')[0].split('_')\n",
    "        if len(title) == 2:\n",
    "            title = title[1]\n",
    "        #tags = row['tags'] if pd.notna(row['tags']) else ' '\n",
    "        text = f\"{row['style_classification']} {row['timeframe_estimation']} {row['artist_school']} {title}\"\n",
    "        \n",
    "        # Truncate text to fit within the context length limit\n",
    "        # text_tokens = clip.tokenize([text])[0]\n",
    "        # if len(text_tokens) > self.context_length:\n",
    "        #     text_tokens = text_tokens[:self.context_length]\n",
    "        #     text = clip.tokenize.decode(text_tokens.tolist())\n",
    "        \n",
    "        return image, text\n",
    "\n",
    "def extract_features(model, dataloader, df, device):\n",
    "    with torch.no_grad():\n",
    "        with open('WikiartCLIP.pkl', 'wb') as file:\n",
    "            for images, texts in dataloader:\n",
    "                images = images.to(device)\n",
    "                texts = clip.tokenize(texts).to(device)\n",
    "\n",
    "                image_features = model.encode_image(images)\n",
    "                text_features = model.encode_text(texts)\n",
    "\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "                text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "                df['clip_image_features'] = list(image_features.cpu())\n",
    "                df['clip_text_features'] = list(text_features.cpu())\n",
    "                pickle.dump(df, file)\n",
    "                #del images, texts  # Free up memory\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_features(model, dataloader, df, device):\n",
    "    image_features_list = []\n",
    "    text_features_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, texts in dataloader:\n",
    "            images = images.to(device)\n",
    "            texts = clip.tokenize(texts).to(device)\n",
    "\n",
    "            image_features = model.encode_image(images)\n",
    "            text_features = model.encode_text(texts)\n",
    "\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            image_features_list.append(image_features.cpu())\n",
    "            text_features_list.append(text_features.cpu())\n",
    "\n",
    "    df['clip_image_features'] = image_features_list\n",
    "    df['clip_text_features'] = text_features_list\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clip_process_data(dataset_name, dataset_path, df, output_path, device):\n",
    "    batch_size = 16\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "    dataset = WikiartDataset(df, dataset_path, preprocess)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    df = df.drop(columns=['concatenated_text', 'tag_prediction', 'title'], errors='ignore')\n",
    "\n",
    "    df = extract_features(model, dataloader,df, device)\n",
    "    df.to_pickle(output_path)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_features(model, dataloader, device):\n",
    "    image_features_list = []\n",
    "    text_features_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, texts in dataloader:\n",
    "            images = images.to(device)\n",
    "            texts = clip.tokenize(texts).to(device)\n",
    "\n",
    "            image_features = model.encode_image(images)\n",
    "            text_features = model.encode_text(texts)\n",
    "\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            image_features_list.append(image_features.cpu())\n",
    "            text_features_list.append(text_features.cpu())\n",
    "\n",
    "    return torch.cat(image_features_list), torch.cat(text_features_list)\n",
    "\n",
    "\n",
    "\n",
    "def clip_process_data(dataset_name, general_path, df, dataset_outpath, device):\n",
    "    batch_size = 16\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "    dataset = WikiartDataset(df, general_path, preprocess)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    image_features, text_features = extract_features(model, dataloader, device)\n",
    "\n",
    "\n",
    "    df['clip_image_features'] = list(image_features)\n",
    "    if dataset_name ==\"wikiart\":\n",
    "        df['clip_text_features'] = list(text_features)\n",
    "    df = df.drop(columns=['concatenated_text', 'tag_prediction', 'title'])\n",
    "    df.to_pickle(dataset_outpath)\n",
    "\n",
    "    return df\n",
    "\n",
    "def main(dataset_name, model=\"ResNet\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "    if dataset_name == 'wikiart':\n",
    "        df_path = 'DATA/Dataset/wikiart/wikiartINFL.pkl'\n",
    "        dataset_path = 'wikiart/'\n",
    "        output_path = 'DATA/Dataset/wikiart/wikiartINFL_clip_1.pkl'\n",
    "    elif dataset_name == 'idesigner':\n",
    "        df_path = 'DATA/Dataset/iDesigner/idesigner_influences_cropped.pkl'\n",
    "        dataset_path = 'DATA/Dataset/iDesigner/designer_image_train_v2_cropped/'\n",
    "        output_path = 'DATA/Dataset/iDesigner/idesignerINFL.pkl'\n",
    "\n",
    "    df = pd.read_pickle(df_path)\n",
    "    df = df.drop(columns=['image_features', 'image_text_features', 'text_features', 'additional_styles'], errors='ignore')\n",
    "\n",
    "    if model == \"clip\":\n",
    "        clip_process_data(dataset_name, dataset_path, df, output_path, device)\n",
    "\n",
    "\n",
    "main(\"wikiart\", \"clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_clip = pd.read_pickle('DATA/Dataset/wikiart/wikiartINFL_clip_1.pkl')\n",
    "\n",
    "df_clip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artsagenet_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
